Before any storage solution is considered whether traditional hardware RAID or software RAID based, like Nexentastor with ZFS, it is critical to determine requirements and accept the fact there are always trade-offs between availability, storage capacity and performance. It is true that we can reconfigure storage after it has been deployed, but consider the challenges and costs of reconfiguring a production Storage Area Network especially when service outages may not be possible in your environment without significant planning, online data migration and coordination with customers. It is critical to understand your storage needs before any purchasing decisions are made to make sure that needs will be met when system is deployed. Nexentastor leverages software RAID with the next-generation ZFS file system. It is important to understand requirements, because without knowing requirements we will not know what trade-offs we should expect to make. There are always trade-offs in any technical decision that we make and having the right knowledge is essential to making choices during the design phase of your Nexentastor solution.

There are several factors that ultimately all play a role in final configuration of the pool and we need address each factor and the decisions made after analyzing each factor. A storage solution may be used very differently by different organizations and one size never fits all. For example, we do not use a Ferrari to tow a boat out to the lake, likewise we do not think of trucks as high-performance roadsters. The same could be applied to storage. For example, when storing backups for periodic recovery our main concern is having enough capacity to retain backups to satisfy retention requirement imposed by the business, with secondary concern being redundancy to properly protect our retained backup data. At the same time, if we are running a latency critical stock-trading application, affect of even moderate latency from improperly configured storage may be completely unacceptable to such an application. 

Complexity in Storage Area Networks is inherent and increases further whenever we begin to mix workloads. If we are using the same storage system for backups and for our highly latency-sensitive trading floor application, we have to make sure that we can maintain our latency requirement and have enough space to handle backup retention requirements all at the same time. Feasible, sure, but it requires careful planning, gathering of requirements and most critically understanding what a suitable storage solution for this environment is. Of course most modern SANs are used to provide storage back-ends for a multitude of applications with potentially very different requirements and knowing this we have to design a system, accepting certain trade-offs, capable of meeting these requirements as well as meeting growth projections, scaling with demands and handling natural spikes in demand, as well as failure of hardware, which inevitably occurs as disks, and any computer equipment fails.

## General Understanding and Key Goals ##

Before we dig any further, there are a few key concepts about ZFS and storage in general that we have to understand and accept, as they will make it easier to further analyze our current storage need and the solution necessary to satisfy this need.

### Latency in the context of storage ###

We touch upon latency in this conversation and without digging deep into the subject matter, which on its own could easily have books written about it, let's briefly address latency. At the high level, when we talk with the context of storage latency is the amount of time it takes to complete IO. It is a key concept which we have to grasp, as it is critical in many high-performance environments today. User experience could very well suffer if their application experiences latency at the storage layer and does not have mechanisms to counter and obscure this latency. These mechanisms are typically some form of caching.

Latency is typically a result of the time it take for the IO request to be structured sent to disk, acknowledged, handled and result returned. Busy storage systems may be doing many thousands of IO operations per second, and in most cases, as is so typical of a SAN environment, IO from a number of applications is mixed together, resulting in what we refer to as random seeks, because the disk in the worst case may have to continuously reposition its heads for every IO request. Latency increases as disks become more busy. When disks are taxed and they are struggling to keep-up, a natural result is an increase in latency. Understanding what latency is and what its impact is in our environment is essential to achieving solid and predictable performance from our storage solution.

Some applications deal with latency more gracefully than others and most environments will have a mix of both kinds. We should always focus on the worst case scenarios, because inevitably they come to pass. We will discuss this in more detail later. This subject could become infinitely complex and so we will try to address some of the basics of latency, but we have to remain cognizant of the fact that understanding latency and its impact to our environment is no trivial task.

Latency is inherent to spinning media because mechanical operations take more time to complete than it takes to move bits and bytes at nearly the speed of light. For this reason we attempt to employ flash-based storage devices as cache, instead of main storage. Flash devices do not, under normal circumstances, suffer from affects of latency, and in fact are specifically built to combat latency at every turn. What makes flash-based storage (Solid State Disks) particularly compelling is their ability to handle random IO extremely well, with incredibly low latency. As always, realize that the more you pay the better the performance you will get. Because predominantly storage systems are asked to handle highly random IO, using flash based storage is an ideal way to combat latency. With ZFS in particular we can of course build solutions entirely out if SSDs, to deal with extreme low-latency requirements. But, more generally we can use SSDs as cache, allowing us to buffer both reads and writes, reducing the impact to disks, resulting on much lower latency while handling large seemingly random workloads. More about this later.

### Key Concepts of ZFS and Dynamic Striping ###

It is difficult to summarize ZFS in a few sentences, but the key concepts that we need to understand are as follows. ZFS first and foremost utilizes dynamic striping, which means that as we create more RAID groups inside our storage pool, or **volume** in Nexentastor terms, we increase throughput by making a wider stripe, which essentially means that we can do more work in parallel. It is easy to visualize each RAID group, also referred to as top-level VDEV as a single disk. So, a pool that consists of say 3 top-level VDEVs regardless of number of disks in each VDEV, can be thought of as having performance of 3 disks. The math is linear. As we add top-level VDEVs we linearly scale performance. To further illustrate this notion we can simply say that a pool with 3 top-level VDEVs will have 1/2 the throughput compared to a pool with 6 top-level VDEVs. This is assuming that we are using the same disks in both of these conceptual configurations. In reality of course, performance varies based on a lot of factors, including the type of disk, it's rotational speed, average seek times, etc. This may sound complicated now, but will begin to make sense as you learn more about Nexentasor and ZFS in particular. The key take away here is that performance scales linearly with number of top-level VDEVs. So, when we think about our storage solution and when we think in terms of IOPs, and not Capacity we have to realize that if we are trying to match IOP performance of 3 disks working in parallel, we have to have at least 3 top-level VDEVs. One is tempted to argue that number of disks in each VDEV surely will make a difference, the key performance factor is parallelism of workload, which we can only achieve by striping data across more top-level VDEVs.

### Aggressive Multi-level Read Caching ###

Another key feature of ZFS, one which makes it extremely performant in today's highly dynamic and diverse environments is the concept of caching. Typically, filesystems are not designed to allocate large caches for data to accelerate repeated retrieval of information, or to buffer writes, such as those generated by databases, which historically have been very difficult on storage systems, due to the synchronous nature of the highly random transactions. With ZFS we try to maximize our ability to rapidly access data which has recently been accessed or is being frequently accessed by caching this data in one of two levels of cache. The first level of cache is system memory, abundance of which means we can allocate larger cache and retain more data and metadata to support larger working set sizes. The second level of cache is flash-based storage, which when deployed on high-performance SSDs will allow for a far larger cache than could be obtained with first level of caching. This is very critical concept, and knowing your workload will help you make decisions about whether or not caching reads will be a significant strategy or only a marginal consideration.

### Write Caching by means of an Intent Log Mechanism ###

Because storage has historically struggled with handling applications that have a requirement of consistent on-disk data by writing information synchronously, built into ZFS is an Intent Log mechanism which essentially allows for synchronous writes to be grouped and ordered along with asynchronous writes and be sent to the disk in a more organized sequential stream, while rapidly responding to the application that is submitting synchronous write requests, allowing the application to continue working instead of waiting for storage to flush data to disk every time IO is sent to the storage system. At the same time ZFS groups IOs in order to sequentially flush them to disk, maximizing throughput and reducing on-disk fragmentation of data. This write cache is accomplished with high-performance NAND flash memory devices, such as SSDs and battery backed RAM devices. These devices do not suffer from latency inherent to spinning disks, and can very rapidly respond to highly random IO, which at an appropriate time is flushed to disks as part of a larger transaction group flush. All writes to disks could be thought of as a transaction and they either all succeed or fail. This fact and presence of the intent log assure data coherency and consistency. 

Again, this is something that we need to think about when we analyze our storage requirements. Many environments today require this level of caching, which is heavily used any time virtualization, OLTP applications and essentially any database engines are being deployed. If the environment has virtualized systems, databases and any applications that you know enforce data integrity by doing sync IO, addition of write cache is practically a requirement and will further benefit the storage solution by reducing amount of IO to the disks allowing the disks to handle other competing IO requests.

### Important goals of this document ###

We cannot forget that in all but a select few environments IO will be generated by multitude of sources and there is ongoing competition by applications to get data. Reads, writes, re-reads, and re-writes always complete for attention from storage and every application is only aware of its own needs and is completely oblivious to the concurrent demands of other applications in the environment. At any moment there may be several applications requesting that data is retrieved from or written to storage. Understanding the fact that we are working with highly dynamic systems that can at a moment's notice change from generating large sequential IO to small highly random IO is critical, and the more data we can aggregate about our particular environment the better equipped we will be in our ongoing research and design of the future storage solution.

The goal of this paper is to equip you with enough information to know what questions to ask during your research and analysis phase and is not intended to be a deep dive into ZFS. A more in-depth discussion about ZFS and its many unique features is strongly encouraged. We are barely scratching the surface of capabilities and intricacies of ZFS.

In general there are a number of key elements that we need to address before we can start to think about our final design. This is a high-level list of these elements, and we will address each in detail.

* What problem are we trying to resolve with the planned storage solution?

* What do the applications in our environment do, and what impact will degraded performance of one application mean to other related applications? Do we have a basic profile of the applications in our environment and do we understand the kind of IO that they generate?

* What are all the applications that will be running on this storage system, and what impact do they have individually as well together. For example, what if some applications are more critical than others, but the less critical applications are particularly tough on storage, generating lots of small random IO?

* Do we want to accept running critical and noncritical applications on same storage realizing the real possibility that noncritical applications may result in performance degradation of the critical application.

* How critical are my applications and data and what is the minimum level of redundancy that will be acceptable to store this data? Level of redundancy has direct impact to capacity and performance.

* How random is the workload, and is the workload mostly static or does it vary with time of day or day of week?

* What is the aggregate ratio of reads to writes, and how much of the same data is read over and over or compared to how much data is read once and not read again for a long period of time, or ever?

* What sort of a growth factor are we dealing with, and is it in capacity IOPs or both.

### Problem Statement ###

Planning for a new storage solution is typically prefaced a need to resolve some existing problem. This problem may be current storage solution's inability to handle growing storage demands, decision to consolidate systems and storage into a more centralized configuration, deployment of some new systems that have storage requirements which you cannot meet with existing storage architecture, etc. Defining the problem is critical, because it will lead into other questions that have to be answered in order to properly scope and configure a new storage system.

It is common for the problem statement to be complex and multifaceted. There is not usually just one well defined problem, and most commonly a combination of issues, for example, consolidation of physical systems into a virtualized clusters, in order to reduce physical footprint and associated costs, while at the same time retiring an aging backup solution that is no longer able to meet your SLAs for completion of backups or perhaps lack of capacity to retain growing data produced by your applications and users.

To successfully define a problem statement we need to ultimately boil things down to the most basic of terms. Essentially, once we strip away most of the details we are looking at a question of Capacity, level of Data Protection and Performance and which two of these three are most important. Remember, we mentioned trade-offs at the beginning of the document, and with Nexentastor, just as with any other storage solution we have to choose between Capacity, Performance and Data Protection. Without understanding our problem we cannot begin to make any decisions about trade-offs that will need to be made. 

For example, if the problem which we are trying to resolve is mainly a lack of capacity to store fairly volatile data, value of which after 30 days diminishes considerably selecting double of triple parity, which of course means sacrificing capacity may not be the best option. If capacity is the main driver and only non-latency critical applications are going to rely on this solution, building a storage pool with only a few stripes may be completely acceptable. We may not even need to think about Read or Write caching, because latency is not issue, we do not have users interacting with the data or applications on this storage, and as long as we can achieve our capacity objective cost always being a factor, the benefit here may not be worth the added cost.

Similarly, if we are trying to achieve higher level of data availability and protection and are willing to sacrifice capacity, we may opt for a greater number of stripes. Having more stripes, or VDEVs actually increases our redundancy, because each VDEV is essentially a RAID group, and with a choice of single or double parity we can afford to lose one or two disks respectively, in each RAID group while still remaining operational. The compromise here is capacity, because as we increase number of VDEVs, we increase number of stripes and in turn amount of raw capacity required to store parity data. We always have to make some trade-off. The added benefit here is an improvement in performance as we parallelize workload. Finding a perfect balance is not easy, but critical.

A situation where we are supplying storage to a highly dynamic Web site that may consist of a multitude of virtualized systems running perhaps a trading floor application or multiple such applications with extreme sensitivity to latency and very high data-availability requirements is completely different from being mainly a data repository, and requires a completely different approach to the design. Here, we may not consider mirroring or single parity, seeking greater data protection provided by double or triple parity. In order to address our critical latency requirements we are likely going to increase number of RAID groups in order to better streamline IO and leverage both Read and Write caching, thereby reducing number of IO requests that have to hit our spinning disks, because inevitably mechanical disk devices, while they have great capacities all suffer from latency induced by positioning and re-positioning the heads. In some very specific use cases we may even consider an all-flash-based pool, again sacrificing capacity for higher price, but achieving outstanding low-latency numbers.

Please bear in mind that these are all extremely simplified examples to help us visualize the challenges with which we will be presented in our own environments. In reality, most of the time we will be dealing with a mixture of workloads some may not be concurrent, for example only occurring during certain times of day, while others being highly concurrent.

### Understanding Applications ###

Our storage system will need to correctly respond to needs of applications in our environment and for that to happen we have to understand what our applications do exactly and how they do it. First we have to accept that by their vary nature SANs are extremely homogeneous, and it is extremely difficult to tell at a specific point in time how storage will behave, because of this homogeneity. In all but very few environments we deploy SANs to centralize and unify storage, taking full advantage of being able to easily share data, reduce waste from having local disks on machines that see little use, increase data redundancy and protection. As we do this though we have remain cognizant of the fact that some applications that used to run on local storage directly attached to a server now have to run on disks at the other end of a wire, competing with other applications that are just as hungry, if not more so for attention from disks. We sometimes wonder why an application that ran extremely well on just a few disks attached to a server is consistently under-performing after being moved to a seemingly far better performing SAN. Ultimately we realize that we took for granted the fact the local disks were completely dedicated to this application, while on the SAN this application becomes one of the many fishes in a big pond competing for time from disks.

It is not enough to understand how one or two applications work if we plan on 10 or more applications to leverage our storage solution. We absolutely have to understand them all. Even same applications in different environments have unique footprints and so it is not meaningful to simply assume that based on some reference document we can tell what the applications do. We need to carefully analyze each application first individually, isolating it against a blank background to get some understanding of just what the application does with regard to IO when there is absolutely no competition. For example, how much IO does our application require at any given moment? Are there times when IO requirements change rapidly, fluctuating due to perhaps other applications in the environment or certain actions that users may be taking. We always have to look at the worst case scenarios and make our decisions based on the worst case scenarios. This is not always feasible, but it is feasible to derive some mean for an application whether it be IO requirements, daily capacity requirements, etc., and reasonably well extrapolate worst case scenario based on experience.
