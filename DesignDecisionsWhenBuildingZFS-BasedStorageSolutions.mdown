Before any storage solution is considered whether traditional hardware RAID or software RAID based, like Nexentastor with ZFS, it is critical to determine requirements and accept the fact there are always trade-offs between availability, storage capacity and performance. It is true that we can reconfigure storage after it has been deployed, but consider the challenges and costs of reconfiguring a production Storage Area Network especially when service outages may not be possible in your environment without significant planning, online data migration and coordination with customers. It is critical to understand your storage needs before any purchasing decisions are made to make sure that needs will be met when system is deployed. Nexentastor leverages software RAID with the next-generation ZFS file system. It is important to understand requirements, because without knowing requirements we will not know what trade-offs we should expect to make. There are always trade-offs in any technical decision that we make and having the right knowledge is essential to making choices during the design phase of your Nexentastor solution.  

There are several factors that ultimately all play a role in final configuration of the pool and we need address each factor and the decisions made after analyzing each factor. A storage solution may be used very differently by different organizations and one size never fits all. For example, we do not use a Ferrari to tow a boat out to the lake, likewise we do not think of trucks as high-performance roadsters. The same could be applied to storage. For example, when storing backups for periodic recovery our main concern is having enough capacity to retain backups to satisfy retention requirement imposed by the business, with secondary concern being redundancy to properly protect our retained backup data. At the same time, if we are running a latency critical stock-trading application, affect of even moderate latency from improperly configured storage may be completely unacceptable to such an application. These faactors play a huge role in our decisions about hardware, and pool configuration with ZFS.  

Complexity in Storage Area Networks is inherent and increases further whenever we begin to mix workloads. If we are using the same storage system for backups and for our highly latency-sensitive trading floor application, we have to make sure that we can maintain our latency requirement and have enough space to handle backup retention requirements all at the same time. Feasible, sure, but it requires careful planning, gathering of requirements and most critically understanding what a suitable storage solution for this environment is. Of course most modern SANs are used to provide storage back-ends for a multitude of applications with potentially very different requirements and knowing this we have to design a system, accepting certain trade-offs, capable of meeting these requirements as well as meeting growth projections, scaling with demands and handling natural spikes in demand, as well as failure of hardware, which inevitably occurs as disks, and any computer equipment fails.  

## General Understanding and Key Goals ##

Before we dig any further, there are a few key concepts about ZFS and storage in general that we have to understand and accept, as they will make it easier to further analyze our current storage need and the solution necessary to satisfy this need.

### Latency in the context of storage ###

We touch upon latency in this conversation and without digging deep into the subject matter, which on its own could easily have books written about it, let's briefly address latency. At the high level, when we talk within the context of storage, we should think of latency as the total amount of time it takes to complete IO, from the moment it has been issued by the application to the moment it has been satisfied. It is a key concept which we have to grasp, as it is critical in many high-performance environments today. User experience could very well suffer if key applications experience latency at the storage layer and does not have mechanisms to counter and obscure this latency. These mechanisms are typically some form of caching. Latency is observed through all the layers of a typical system. It takes time for an application to prepare and submit an IO. It takes time for that IO to be properly structured and sent to the filesystem, and for filesystem to process it perform actual IO to physical storage and return results. When we decouple storage from the system where the application resides, be it a single bare-metal system, or a virtual host with many VMs running on it, we also introduce latency at the network layer.

Latency is typically a result of the total time it takes for the IO request to be structured, perhaps delivered via the network, sent to disk, acknowledged, handled and result returned. Busy storage systems may be doing many thousands of IO operations per second, and in most cases, as is so typical of a SAN environment, IO from a number of applications is mixed together, resulting in a highly random pattern. Worst case scenario, which is quite common with homogeneous SANs is a 100% random workload which necessitates very frequent repositioning of the heads in a disk drive. Of course as is typical of most SANs, this is happening to many disks at the same time. Latency increases as disks become more busy. Data is typically queued in multiple places between the application (source of the IO) and the physical disks. Time spent in one of these queues will in fact add to the total latency number. 

Environments where virtualization is leveraged are even more susceptible to latency induced by storage, because there are more points at which IO contention can occur and where IO could be queued. Virtual machines running on a host will send their IOs to the host, and during peak times may in fact be saturating the host with IO requests, resulting in the host relying on its own often deep queues to hang on to IOs that it is unable to service right away. This gets infinitely more complicated when we begin to think about affect of peak times in systems that host virtual machines and the amount of IO that may be generated during these peak times.

When disks are taxed and they are struggling to keep-up, a natural result is an increase in latency. Flooding disks with IO, especially with modern SANs on multi-link 10GbE systems is not difficult. Understanding what latency is and what its impact is in our environment is essential to achieving solid and predictable performance from our storage solution.

Some applications deal with latency more gracefully than others and most environments will have a mix of applications, some of which handle it well, and some do not. We should always focus on the worst case scenarios, the outliers, because inevitably the worst case scenarios are those we are least prepared for. We will discuss this in more detail later. This subject could become infinitely complex and so we will try to address some of the basics of latency, but we have to remain cognizant of the fact that understanding latency and its impact to our environment is no trivial task.

### ZFS Approach to dealing with latency ###

Latency is inherent to spinning media because mechanical operations take more time to complete than it takes to move bits at nearly the speed of light. For this reason we attempt to employ flash-based storage devices as cache in front of main storage, which predominantly is spinning media. Flash devices do not, under normal circumstances, suffer from affects of latency, and in fact are specifically built to combat latency at every turn. What makes flash-based storage (Solid State Disks) particularly compelling is their ability to handle random IO extremely well, with incredibly low latency. Because highly random IO is so tough on spinning disks, pushing some of that work onto cache is one of ZFS' primary strengths. As always, realize that the more you pay the better the performance you will get. Because predominantly storage systems are asked to handle highly random IO, using flash based storage is an ideal way to combat latency. With ZFS in particular we can of course build solutions entirely out if SSDs, to deal with extreme low-latency requirements. But, more generally we can use SSDs as cache, allowing us to buffer both reads and writes, reducing the number of physical IO to disks, resulting in much lower latency while handling large very random workloads. More about this later.

Write throttling is another absolutely essential concept that is not native to many filesystems, but is a real factor when dealing with ZFS and Nexentastor in particular. Because latency is so critical, and we understand just how critical it is, we have ways to make sure that writes do not completely overwhelm storage and really impact read performance. Throttling writes is the approach we take to make sure that reads will complete in reasonable amount of time and not suffer significant latency costs. This can become rather problematic in environments where workloads shift drastically from being heavily write biased to read biased. Throttling of writes could in fact become so severe that a SAN may seem completely overrun, yet performance tools show almost no IO. This is a factor which we have to take into account and as we plan our environment we have to make sure that we do not constrain it from day one and design it for our specific workload footprint. Again, understanding our environment is key, and as we work to design our solution we need to think about whether we are going to be read or write-biased and design accordingly. While we are not diving deeply into this topic, this should at least give you the knowledge to ask questions that you did not know to ask prior to reading this. Ultimately our goal is get you to think in terms of storage and in particular ZFS, because with traditional (legacy) storage we did not have these factors to consider.

### Key Concepts of ZFS and Dynamic Striping ###

It is difficult to summarize ZFS in a few sentences, but the key concepts that we need to understand are as follows. ZFS first and foremost utilizes dynamic striping, which means that as we create more RAID groups inside our storage pool, or **volume** in Nexentastor terms, we increase throughput by making a wider stripe, which essentially means that we can do more work in parallel. It is easy to visualize each RAID group, also referred to as top-level VDEV as a single disk. So, a pool that consists of say 3 top-level VDEVs regardless of number of disks in each VDEV, can be thought of as having performance of 3 disks. The math is linear. As we add top-level VDEVs we linearly scale performance. To further illustrate this notion we can simply say that a pool with 3 top-level VDEVs will have 1/2 the throughput compared to a pool with 6 top-level VDEVs, and 1/4 the troughput of a 12 top-level VDEV pool. This is assuming that we are using the same disks in these conceptual configurations. In reality of course, performance varies based on a lot of factors, including the type of disk, it's rotational speed, average seek times, etc. This may sound complicated now, but will begin to make sense as you learn more about Nexentasor and ZFS in particular. The key take away here is that performance scales linearly with number of top-level VDEVs. So, when we think about our storage solution and when we think in terms of IOPs, and not Capacity we have to realize that if we are trying to match IOP performance of 3 disks working in parallel, we have to have at least 3 top-level VDEVs. One is tempted to argue that number of disks in each VDEV surely will make a difference, the key performance factor is parallelism of workload, which we can only achieve by striping data across more top-level VDEVs.

### Aggressive Multi-level Read Caching ###

Another key feature of ZFS, one which makes it extremely performant in today's highly dynamic and diverse environments is the concept of a massive read cache. Historically, filesystems were not designed to allocate large caches for data to accelerate repeated retrieval of information, or to buffer writes, such as those generated by databases, which are particularly difficult on storage systems, due to the synchronous nature of the highly random transactions. With ZFS we try to maximize our ability to rapidly access data which has recently been accessed or is being frequently accessed by caching this data in one of two levels of cache. The first level of cache is system memory, abundance of which means we can allocate more physical blocks of memory to cache and retain more data and metadata to support larger working set sizes. Of course RAM is still a fairly limited commodity The second level of cache is flash-based storage, which when deployed on high-performance SSDs will allow for a far larger cache than could be obtained with first level of caching. This is very critical concept, and knowing your workload will help you make decisions about whether or not caching reads will be a significant strategy or only a marginal consideration.

### Write Caching by means of an Intent Log Mechanism ###

Because storage has historically struggled with handling applications that have a requirement of consistent on-disk data by writing information synchronously, built into ZFS is an Intent Log mechanism which essentially allows for synchronous writes to be grouped and ordered along with asynchronous writes and be sent to the disk in a more organized sequential stream, while rapidly responding to the application that is submitting synchronous write requests, allowing the application to continue working instead of waiting for storage to flush data to disk every time IO is sent to the storage system. At the same time ZFS groups IOs in order to sequentially flush them to disk, maximizing throughput and reducing on-disk fragmentation of data. This write cache is accomplished with high-performance NAND flash memory devices, such as SSDs and battery backed RAM devices. These devices do not suffer from latency inherent to spinning disks, and can very rapidly respond to highly random IO, which at an appropriate time is flushed to disks as part of a larger transaction group flush. All writes to disks could be thought of as a transaction and they either all succeed or fail. This fact and presence of the intent log assure data coherency and consistency. 

Again, this is something that we need to think about when we analyze our storage requirements. Many environments today require this level of caching, which is heavily used any time virtualization, OLTP applications and essentially any database engines are being deployed. If the environment has virtualized systems, databases and any applications that you know enforce data integrity by doing sync IO, addition of write cache is practically a requirement and will further benefit the storage solution by reducing amount of IO to the disks allowing the disks to handle other competing IO requests.

### Important goals of this document ###

We cannot forget that in all but a select few environments IO will be generated by multitude of sources and there is ongoing competition by applications to get data. Reads, writes, re-reads, and re-writes always complete for attention from storage and every application is only aware of its own needs and is completely oblivious to the concurrent demands of other applications in the environment. At any moment there may be several applications requesting that data is retrieved from or written to storage. Understanding the fact that we are working with highly dynamic systems that can at a moment's notice change from generating large sequential IO to small highly random IO is critical, and the more data we can aggregate about our particular environment the better equipped we will be in our ongoing research and design of the future storage solution.

The goal of this paper is to equip you with enough information to know what questions to ask during your research and analysis phase and is not intended to be a deep dive into ZFS. A more in-depth discussion about ZFS and its many unique features is strongly encouraged. We are barely scratching the surface of capabilities and intricacies of ZFS.

In general there are a number of key elements that we need to address before we can start to think about our final design. This is a high-level list of these elements, and we will address each in detail.

* What problem are we trying to resolve with the planned storage solution?

* What do the applications in our environment do, and what impact will degraded performance of one application mean to other related applications? Do we have a basic profile of the applications in our environment and do we understand the kind of IO that they generate?

* What are all the applications that will be running on this storage system, and what impact do they have individually as well together. For example, what if some applications are more critical than others, but the less critical applications are particularly tough on storage, generating lots of small random IO?

* Do we want to accept running critical and noncritical applications on same storage realizing the real possibility that noncritical applications may result in performance degradation of the critical application.

* How critical are my applications and data and what is the minimum level of redundancy that will be acceptable to store this data? Level of redundancy has direct impact to capacity and performance.

* How random is the workload, and is the workload mostly static or does it vary with time of day or day of week?

* What is the aggregate ratio of reads to writes, and how much of the same data is read over and over or compared to how much data is read once and not read again for a long period of time, or ever?

* What sort of a growth factor are we dealing with, and is it in capacity IOPs or both.

### Problem Statement ###

Planning for a new storage solution is typically prefaced a need to resolve some existing problem. This problem may be current storage solution's inability to handle growing storage demands, decision to consolidate systems and storage into a more centralized configuration, deployment of some new systems that have storage requirements which you cannot meet with existing storage architecture, etc. Defining the problem is critical, because it will lead into other questions that have to be answered in order to properly scope and configure a new storage system.

It is common for the problem statement to be complex and multifaceted. There is not usually just one well defined problem, and most commonly a combination of issues, for example, consolidation of physical systems into a virtualized clusters, in order to reduce physical footprint and associated costs, while at the same time retiring an aging backup solution that is no longer able to meet your SLAs for completion of backups or perhaps lack of capacity to retain growing data produced by your applications and users.

To successfully define a problem statement we need to ultimately boil things down to the most basic of terms. Essentially, once we strip away most of the details we are looking at a question of Capacity, level of Data Protection and Performance and which two of these three are most important. Remember, we mentioned trade-offs at the beginning of the document, and with Nexentastor, just as with any other storage solution we have to choose between Capacity, Performance and Data Protection. Without understanding our problem we cannot begin to make any decisions about trade-offs that will need to be made. 

For example, if the problem which we are trying to resolve is mainly a lack of capacity to store fairly volatile data, value of which after 30 days diminishes considerably selecting double of triple parity, which of course means sacrificing capacity may not be the best option. If capacity is the main driver and only non-latency critical applications are going to rely on this solution, building a storage pool with only a few stripes may be completely acceptable. We may not even need to think about Read or Write caching, because latency is not issue, we do not have users interacting with the data or applications on this storage, and as long as we can achieve our capacity objective cost always being a factor, the benefit here may not be worth the added cost.

Similarly, if we are trying to achieve higher level of data availability and protection and are willing to sacrifice capacity, we may opt for a greater number of stripes. Having more stripes, or VDEVs actually increases our redundancy, because each VDEV is essentially a RAID group, and with a choice of single or double parity we can afford to lose one or two disks respectively, in each RAID group while still remaining operational. The compromise here is capacity, because as we increase number of VDEVs, we increase number of stripes and in turn amount of raw capacity required to store parity data. We always have to make some trade-off. The added benefit here is an improvement in performance as we parallelize workload. Finding a perfect balance is not easy, but critical.

A situation where we are supplying storage to a highly dynamic Web site that may consist of a multitude of virtualized systems running perhaps a trading floor application or multiple such applications with extreme sensitivity to latency and very high data-availability requirements is completely different from being mainly a data repository, and requires a completely different approach to the design. Here, we may not consider mirroring or single parity, seeking greater data protection provided by double or triple parity. In order to address our critical latency requirements we are likely going to increase number of RAID groups in order to better streamline IO and leverage both Read and Write caching, thereby reducing number of IO requests that have to hit our spinning disks, because inevitably mechanical disk devices, while they have great capacities all suffer from latency induced by positioning and re-positioning the heads. In some very specific use cases we may even consider an all-flash-based pool, again sacrificing capacity for higher price, but achieving outstanding low-latency numbers.

Please bear in mind that these are all extremely simplified examples to help us visualize the challenges with which we will be presented in our own environments. In reality, most of the time we will be dealing with a mixture of workloads some may not be concurrent, for example only occurring during certain times of day, while others being highly concurrent.

### Understanding Applications ###

Our storage system will need to correctly respond to needs of applications in our environment and for that to happen we have to understand what our applications do exactly and how they do it. First we have to accept that by their vary nature SANs are extremely homogeneous, and it is extremely difficult to tell at a specific point in time how storage will behave, because of this homogeneity. In all but very few environments we deploy SANs to centralize and unify storage, taking full advantage of being able to easily share data, reduce waste from having local disks on machines that see little use, increase data redundancy and protection. As we do this though we have remain cognizant of the fact that some applications that used to run on local storage directly attached to a server now have to run on disks at the other end of a wire, competing with other applications that are just as hungry, if not more so for attention from disks. We sometimes wonder why an application that ran extremely well on just a few disks attached to a server is consistently under-performing after being moved to a seemingly far better performing SAN. Ultimately we realize that we took for granted the fact the local disks were completely dedicated to this application, while on the SAN this application becomes one of the many fishes in a big pond competing for time from disks.

It is not enough to understand how one or two applications work if we plan on 10 or more applications to leverage our storage solution. We absolutely have to understand them all. Even same applications in different environments have unique footprints and so it is not meaningful to simply assume that based on some reference document we can tell what the applications do. We need to carefully analyze each application first individually, isolating it against a blank background to get some understanding of just what the application does with regard to IO when there is absolutely no competition. For example, how much IO does our application require at any given moment? Are there times when IO requirements change rapidly, fluctuating due to perhaps other applications in the environment or certain actions that users may be taking. We always have to look at the worst case scenarios and make our decisions based on the worst case scenarios. This is not always feasible, but it is feasible to derive some mean for an application whether it be IO requirements, daily capacity requirements, etc., and reasonably well extrapolate worst case scenario based on experience.
